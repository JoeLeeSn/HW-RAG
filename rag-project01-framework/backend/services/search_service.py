from typing import List, Dict, Any, Optional
import logging
from datetime import datetime
from pymilvus import connections, Collection, utility
from services.embedding_service import EmbeddingService
from utils.config import VectorDBProvider, MILVUS_CONFIG
import os
import json

logger = logging.getLogger(__name__)

class SearchService:
    """
    æœç´¢æœåŠ¡ç±»ï¼Œè´Ÿè´£å‘é‡æ•°æ®åº“çš„è¿æ¥å’Œå‘é‡æœç´¢åŠŸèƒ½
    æä¾›é›†åˆåˆ—è¡¨æŸ¥è¯¢ã€å‘é‡ç›¸ä¼¼åº¦æœç´¢å’Œæœç´¢ç»“æœä¿å­˜ç­‰åŠŸèƒ½
    """
    def __init__(self):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡
        åˆ›å»ºåµŒå…¥æœåŠ¡å®ä¾‹ï¼Œè®¾ç½®Milvusè¿æ¥URIï¼Œåˆå§‹åŒ–æœç´¢ç»“æœä¿å­˜ç›®å½•
        """
        self.embedding_service = EmbeddingService()
        self.milvus_uri = MILVUS_CONFIG["uri"]
        self.search_results_dir = "04-search-results"
        os.makedirs(self.search_results_dir, exist_ok=True)

    def get_providers(self) -> List[Dict[str, str]]:
        """
        è·å–æ”¯æŒçš„å‘é‡æ•°æ®åº“åˆ—è¡¨
        
        Returns:
            List[Dict[str, str]]: æ”¯æŒçš„å‘é‡æ•°æ®åº“æä¾›å•†åˆ—è¡¨
        """
        return [
            {"id": VectorDBProvider.MILVUS.value, "name": "Milvus"}
        ]

    def list_collections(self, provider: str = VectorDBProvider.MILVUS.value) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šå‘é‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰é›†åˆ
        
        Args:
            provider (str): å‘é‡æ•°æ®åº“æä¾›å•†ï¼Œé»˜è®¤ä¸ºMilvus
            
        Returns:
            List[Dict[str, Any]]: é›†åˆä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«idã€åç§°å’Œå®ä½“æ•°é‡
            
        Raises:
            Exception: è¿æ¥æˆ–æŸ¥è¯¢é›†åˆæ—¶å‘ç”Ÿé”™è¯¯
        """
        try:
            connections.connect(
                alias="default",
                uri=self.milvus_uri
            )
            
            collections = []
            collection_names = utility.list_collections()
            
            for name in collection_names:
                try:
                    collection = Collection(name)
                    collections.append({
                        "id": name,
                        "name": name,
                        "count": collection.num_entities
                    })
                except Exception as e:
                    logger.error(f"Error getting info for collection {name}: {str(e)}")
            
            return collections
            
        except Exception as e:
            logger.error(f"Error listing collections: {str(e)}")
            raise
        finally:
            connections.disconnect("default")

    def save_search_results(self, query: str, collection_id: str, results: List[Dict[str, Any]]) -> str:
        """
        ä¿å­˜æœç´¢ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            query (str): æœç´¢æŸ¥è¯¢æ–‡æœ¬
            collection_id (str): é›†åˆID
            results (List[Dict[str, Any]]): æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            str: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
            
        Raises:
            Exception: ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            filename = f"search_{collection_id}_{timestamp}.json"
            filepath = os.path.join(self.search_results_dir, filename)
            
            search_data = {
                "query": query,
                "collection_id": collection_id,
                "timestamp": datetime.now().isoformat(),
                "results": results
            }
            
            logger.info(f"Attempting to save results to: {filepath}")
            logger.debug(f"Results data sample: {json.dumps(results[0], indent=2)[:200]}...")
            
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(search_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Successfully saved results to: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error saving results: {str(e)}", exc_info=True)
            raise

    async def search(self, 
                    query: str, 
                    collection_id: str, 
                    top_k: int = 3, 
                    threshold: float = 0.7,
                    word_count_threshold: int = 20,
                    save_results: Any = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œå‘é‡æœç´¢
        
        Args:
            query (str): æœç´¢æŸ¥è¯¢æ–‡æœ¬
            collection_id (str): è¦æœç´¢çš„é›†åˆID
            top_k (int): è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º3
            threshold (float): ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤ï¼Œé»˜è®¤ä¸º0.7
            word_count_threshold (int): æ–‡æœ¬å­—æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤ï¼Œé»˜è®¤ä¸º20
            save_results (bool): æ˜¯å¦ä¿å­˜æœç´¢ç»“æœï¼Œé»˜è®¤ä¸ºFalse
            
        Returns:
            Dict[str, Any]: åŒ…å«æœç´¢ç»“æœçš„å­—å…¸ï¼Œå¦‚æœä¿å­˜ç»“æœåˆ™åŒ…å«ä¿å­˜è·¯å¾„
            
        Raises:
            Exception: æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
        """
        try:
            # å¼ºåˆ¶ç±»å‹è½¬æ¢å’ŒéªŒè¯
            if isinstance(save_results, str):
                save_results = save_results.lower() in ['true', '1', 't']
            save_results = bool(save_results)
            logger.info(f"è½¬æ¢åçš„save_results: {save_results} (type: {type(save_results)})")
            
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(self.search_results_dir, exist_ok=True)
            
            # æ·»åŠ å‚æ•°æ—¥å¿—
            logger.info(f"Search parameters:")
            logger.info(f"- Query: {query}")
            logger.info(f"- Collection ID: {collection_id}")
            logger.info(f"- Top K: {top_k}")
            logger.info(f"- Threshold: {threshold}")
            logger.info(f"- Word Count Threshold: {word_count_threshold}")
            logger.info(f"- Save Results: {save_results} (type: {type(save_results)}, value: {save_results})")

            logger.info(f"Starting search with parameters - Collection: {collection_id}, Query: {query}, Top K: {top_k}")
            
            # è¿æ¥åˆ° Milvus
            logger.info(f"Connecting to Milvus at {self.milvus_uri}")
            connections.connect(
                alias="default",
                uri=self.milvus_uri
            )
            
            # è·å–collection
            logger.info(f"Loading collection: {collection_id}")
            collection = Collection(collection_id)
            collection.load()
            
            # è®°å½•collectionçš„åŸºæœ¬ä¿¡æ¯
            logger.info(f"Collection info - Entities: {collection.num_entities}")
            
            # ä»collectionä¸­è¯»å–embeddingé…ç½®
            logger.info("Querying sample entity for embedding configuration")
            sample_entity = collection.query(
                expr="id >= 0", 
                output_fields=["embedding_provider", "embedding_model"],
                limit=1
            )
            if not sample_entity:
                logger.error(f"Collection {collection_id} is empty")
                raise ValueError(f"Collection {collection_id} is empty")
            
            logger.info(f"Sample entity configuration: {sample_entity[0]}")
            
            # ä½¿ç”¨collectionä¸­å­˜å‚¨çš„é…ç½®åˆ›å»ºæŸ¥è¯¢å‘é‡
            logger.info("Creating query embedding")
            query_embedding = self.embedding_service.create_single_embedding(
                query,
                provider=sample_entity[0]["embedding_provider"],
                model=sample_entity[0]["embedding_model"]
            )
            logger.info(f"Query embedding created with dimension: {len(query_embedding)}")
            
            # æ‰§è¡Œæœç´¢
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": 10}
            }
            logger.info(f"Executing search with params: {search_params}")
            logger.info(f"Word count threshold filter: word_count >= {word_count_threshold}")
            
            results = collection.search(
                data=[query_embedding],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                expr=f"word_count >= {word_count_threshold}",
                output_fields=[
                    "content",
                    "document_name",
                    "chunk_id",
                    "total_chunks",
                    "word_count",
                    "page_number",
                    "page_range",
                    "embedding_provider",
                    "embedding_model",
                    "embedding_timestamp"
                ]
            )
            
            # å¤„ç†ç»“æœ
            processed_results = []
            logger.info(f"Raw search results count: {len(results[0])}")
            
            for hits in results:
                for hit in hits:
                    word_count = hit.entity.get('word_count') or len(hit.entity.content.split())
                    logger.info(f"Processing hit - Score: {hit.score}, Word Count: {word_count} (åŸå§‹å€¼: {hit.entity.get('word_count')})")
                    if hit.score >= threshold and word_count >= word_count_threshold:
                        processed_results.append({
                            "text": hit.entity.content,
                            "score": float(hit.score),
                            "metadata": {
                                "source": hit.entity.document_name,
                                "page": hit.entity.page_number,
                                "chunk": hit.entity.chunk_id,
                                "total_chunks": hit.entity.total_chunks,
                                "page_range": hit.entity.page_range,
                                "embedding_provider": hit.entity.embedding_provider,
                                "embedding_model": hit.entity.embedding_model,
                                "embedding_timestamp": hit.entity.embedding_timestamp
                            }
                        })

            logger.info(f"è¿‡æ»¤åæœ‰æ•ˆç»“æœæ•°é‡: {len(processed_results)}")

            response_data = {"results": processed_results}
            
            # ä¿å­˜ç»“æœéƒ¨åˆ†
            if save_results:
                logger.info(
                    "ğŸ”„ è§¦å‘è‡ªåŠ¨ä¿å­˜ | "
                    f"å‚æ•°çŠ¶æ€: save_results={save_results} | "
                    f"æœ‰æ•ˆç»“æœæ•°={len(processed_results)} | "
                    f"ä¿å­˜ç›®å½•={self.search_results_dir}"
                )
                
                if processed_results:
                    try:
                        logger.info(f"ğŸ“ æ­£åœ¨åˆ›å»ºä¿å­˜æ–‡ä»¶ | é›†åˆID={collection_id} | æŸ¥è¯¢å†…å®¹='{query}'")
                        filepath = self.save_search_results(
                            query=query,
                            collection_id=collection_id,
                            results=processed_results
                        )
                        response_data["saved_filepath"] = filepath
                        logger.info(f"âœ… ä¿å­˜æˆåŠŸ | è·¯å¾„={filepath} | æ–‡ä»¶å¤§å°={os.path.getsize(filepath)}å­—èŠ‚")
                    except Exception as e:
                        logger.error(
                            f"âŒ ä¿å­˜å¤±è´¥ | é”™è¯¯ç±»å‹={type(e).__name__} | "
                            f"é”™è¯¯ä¿¡æ¯={str(e)} | å †æ ˆè·Ÿè¸ªè¯¦è§æ—¥å¿—",
                            exc_info=True
                        )
                        response_data["save_error"] = f"ä¿å­˜å¤±è´¥: {str(e)}"
                else:
                    logger.warning(
                        "âš ï¸ è·³è¿‡ä¿å­˜ | åŸå› : processed_resultsä¸ºç©º | "
                        f"åŸå§‹ç»“æœæ•°={len(results[0])} | "
                        f"è¿‡æ»¤é˜ˆå€¼={threshold}/{word_count_threshold}"
                    )
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error performing search: {str(e)}")
            raise
        finally:
            connections.disconnect("default") 